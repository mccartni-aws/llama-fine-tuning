{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host QLoRA Model for Inference with AWS Inf2 using SageMaker LMI Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Upgrade SageMaker and reload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet\n",
    "\n",
    "# make sure updates to the python modules are imported\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Imports and SageMaker Session & Default Bucket instantiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import serializers\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Write service properties to local file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=s3://<your_bucket_name_here>/tallrec-training-2024-06-11-13-54-43-286/output/merged_model/\n",
    "option.batch_size=4\n",
    "option.neuron_optimize_level=2\n",
    "option.tensor_parallel_degree=8\n",
    "option.n_positions=512\n",
    "option.rolling_batch=auto\n",
    "option.dtype=fp16\n",
    "option.model_loading_timeout=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create a working directory to compress the serving properties into**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir neuron_model_inf2\n",
    "mv serving.properties neuron_model_inf2/\n",
    "tar -czvf neuron_model_inf2.tar.gz neuron_model_inf2/\n",
    "rm -rf neuron_model_inf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Upload to S3 and remove original, local tar.gz file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "code_artifact = sess.upload_data(\"neuron_model_inf2.tar.gz\", bucket, s3_code_prefix)\n",
    "\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "!rm -rf neuron_model_inf2.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Get the appropriate ECR image URI for serving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-neuronx\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.24.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Define our instance type and endpoint name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inf2 instance type to use for serving\n",
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "\n",
    "endpoint_name = \"llama2\" + sagemaker.utils.name_from_base(\"lmi-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Deploy our model to a SageMaker endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Model object with the image and model data\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             container_startup_health_check_timeout=1500,\n",
    "             volume_size=256,\n",
    "             endpoint_name=endpoint_name\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9: Test inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and extract the answers from the generated responses\n",
    "def extract_answer(response_text):\n",
    "    # Split the response on 'Response:' and take the last part\n",
    "    response_part = response_text.split('Response:')[-1].strip()\n",
    "    # Extract the first line as the answer\n",
    "    answer_line = response_part.split('\\n')[0].strip()\n",
    "    return answer_line\n",
    "\n",
    "# Define the generate_prompt function\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction: {instruction}\n",
    "### Input: {input}\n",
    "### Response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  # noqa: E501 \n",
    "### Instruction: {instruction}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Load the test data from the JSON file\n",
    "test_data_path = 'datasets/book/test.json'\n",
    "with open(test_data_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Limit the data to the first 500 samples\n",
    "test_data = test_data[:500]\n",
    "\n",
    "print(f\"Loaded test dataset containing {len(test_data)} items.\")\n",
    "\n",
    "# Extract instructions and inputs\n",
    "instructions = [_['instruction'] for _ in test_data]\n",
    "inputs = [_['input'] for _ in test_data]\n",
    "\n",
    "# Generate prompts\n",
    "prompts = [generate_prompt(instruction, input) for instruction, input in zip(instructions, inputs)]\n",
    "\n",
    "# Define the generation parameters\n",
    "generation_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 40,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 256\n",
    "}\n",
    "\n",
    "# Make predictions using the fine-tuned model\n",
    "responses = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    if i >= 15:\n",
    "        break\n",
    "    response = predictor.predict(\n",
    "        {\"inputs\": prompt, \"parameters\": generation_parameters}\n",
    "    )\n",
    "    generated_response = json.loads(response)['generated_text']\n",
    "    responses.append(generated_response)\n",
    "\n",
    "# Extracted answers\n",
    "extracted_answers = [extract_answer(response) for response in responses]\n",
    "\n",
    "# Print the results with additional spacing\n",
    "for i, (instruction, input, response, answer) in enumerate(zip(instructions[:15], inputs[:15], responses, extracted_answers), 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Instruction:\\n{instruction}\\n\")\n",
    "    print(f\"Input:\\n{input}\\n\")\n",
    "    print(f\"Generated Response:\\n{response}\\n\")\n",
    "    print(f\"Extracted Answer:\\n{answer}\\n\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 10: Build Gradio application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Combine instructions and inputs for dropdown options\n",
    "examples = [f\"Instruction: {instr}\\nInput: {inp}\" for instr, inp in zip(instructions, inputs)]\n",
    "\n",
    "def query_endpoint(selected_example, endpoint_name, generation_parameters):\n",
    "    smr = boto3.client(\"sagemaker-runtime\")\n",
    "    \n",
    "    # Extract instruction and input from the selected example\n",
    "    instruction, input_data = selected_example.split('Input:', 1)\n",
    "    instruction = instruction.replace('Instruction:', '').strip()\n",
    "    input_data = input_data.strip()\n",
    "    \n",
    "    # Generate the prompt\n",
    "    formatted_prompt = generate_prompt(instruction, input_data)\n",
    "    \n",
    "    request_body = {\n",
    "        \"inputs\": formatted_prompt,\n",
    "        \"parameters\": generation_parameters\n",
    "    }\n",
    "    \n",
    "    response = smr.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(request_body),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    raw_text = response_body.get('generated_text', 'No response')\n",
    "    \n",
    "    # Clean the output using BeautifulSoup to remove HTML tags\n",
    "    soup = BeautifulSoup(raw_text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text()\n",
    "    \n",
    "    # Further cleanup if necessary\n",
    "    cleaned_text = cleaned_text.replace(\"</s>\", \"\").strip()\n",
    "\n",
    "    answer = extract_answer(cleaned_text)\n",
    "    return answer\n",
    "\n",
    "# Define your generation_parameters\n",
    "generation_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 40,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 128\n",
    "}\n",
    "\n",
    "# Define the Gradio interface\n",
    "def gradio_interface(selected_example):\n",
    "    if isinstance(selected_example, list) and len(selected_example) > 0:\n",
    "        selected_example = selected_example[0]\n",
    "    elif isinstance(selected_example, list):\n",
    "        return \"No example selected\"\n",
    "    return query_endpoint(selected_example, endpoint_name, generation_parameters)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Dropdown(label=\"Select an example\", choices=examples),\n",
    "    outputs=\"text\",\n",
    "    title=\"TallRec Book Recommender on AWS Inferentia\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
